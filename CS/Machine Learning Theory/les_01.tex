\chapter{Foundations}
\section{Definitions and Finite hypothesis classes} \label{sec:}

\subsection{A Formal Model} \label{sec:}

\begin{definition}[Domain set]
    An arbitrary set $\mathcal{X}$, the set of objects that we may wish to label.
\end{definition}
\begin{definition}[Label set]
    (temporary) A two-element set, usually $\{ 0 , 1 \}$, let $\mathcal{Y}$ denote the \textbf{set of possible labels}.
\end{definition}

\subsubsection{The learner's input:}
\begin{definition}[Training data]
$S = ((x_1,y_1)\cdots (x_m,y_m))$ is a finite sequence of pairs in $\mathcal{X} \times \mathcal{Y}$, that is, a sequence of labeled domain points.
\end{definition}

\subsubsection{A simple data-generation model:}
\begin{enumerate}
    \item First we have some probability distribution over $\mathcal{X}$ denoted by $\mathcal{D}$.
    \item (temporary) Also we assume there exist some true labeling function, $f: \mathcal{X} \to \mathcal{Y}$, such that $y_i = f(x_i)$ for all $i$, and it is unknown to the learner.
    \item Each pair in the training dataset is generated by first sampling a point $x_i$ according to $\mathcal{D}$ and the labeling it by the true labeling function $f$.
\end{enumerate}

\subsubsection{The learner's output:}
\begin{definition}[Hypothesis]
Hypothesis: $h: \mathcal{X} \to \mathcal{Y}$.
\end{definition}

\subsubsection{Measure of success:}
\begin{definition}[True Risk] ~

The probability that it does not predict the correct label on a random data point generated by the aforementioned underlying distribution $\mathcal{D}$:
\begin{align*}
    L_{\mathcal{D}, f}(h) := \underset{x \sim \mathcal{D}}{\mathbb{P}} [h(x) \neq f(x)] := \mathcal{D}(\{ x: h(x) \neq f(x) \})
\end{align*}
Also means that the error is the probability of randomly choosing an example $x$ for which $h(x)\neq f(x)$.
\end{definition}

\begin{notation}
    Denote $A(S)$ as the hypothesis that a learning algorithm $A$ returns upon receiving the training data $S$.
\end{notation}

\begin{summary}
    Begin with training data, feed into the learning algorithm $A(S)$, and output the hypothesis $h$ used for predicting. Then measure the success of $h$ using loss function $L_{\mathcal{D}, f}(h)$ which depends on the distribution $\mathcal{D}$ and the ture label function $f$, which tells us the probability of prediction a wrong label on a random data point.
\end{summary}

\subsection{Empirical Risk Minimization} \label{sec:}
\begin{definition}[Empirical Risk; Training error]
    \begin{align*}
        L_S(h) := \frac{|\{ i \in [m] : h(x_i) \neq  y_i \}|}{m}
    \end{align*}
    where $[m] = \{  1, \cdots , m \}$.
\end{definition}

\begin{intuition}
    Training error is the probability making wrong prediction within the training dataset.
\end{intuition}

\begin{definition}[Empirical Risk Minimization]
The process of finding $h$ that minimizing $L_S(h)$ is called \textbf{Empirical Risk Minimization}, and we such $h$ the \textbf{ERM hypothesis}.
\end{definition}

\subsection{Empirical Risk Minimization with Inductive Bias} \label{sec:}

\begin{definition}[Hypothesis class]
\textbf{Hypothesis class} ($\mathcal{H}$) is the set of hypothesis that the learner choose in advance without have any knowledge about the training set, but might depend on some prior knowledge about the problem to be learned.
\end{definition}

\begin{definition}[Inductive bias]
\textbf{Inductive bias} is the restriction that we only allow the learner to choose hypothesis from $\mathcal{H}$.
\end{definition}

\begin{definition}[ERM with Inductive Bias]
$\text{ERM}_\mathcal{H}$ is the process of finding
    \begin{align*}
        h_S \in \argmin_{h \in \mathcal{H}} L_S(h)
    \end{align*} 
\end{definition}

\begin{notation}
    Let $h_S$ denote a result of applying $\text{ERM}_\mathcal{H}$ to $S$.
\end{notation}

\begin{remark} ~
    \begin{itemize}
        \item A \textbf{fundamental questions} is: over which hypothesis classes $\text{ERM}_\mathcal{H}$ learning will not result in overfitting?
        \item A \textbf{fundamental trade-off} is: choosing a more restricted hypothesis class better protects us against overfitting but at the same time might cause us a stronger inductive bias. 
    \end{itemize}
\end{remark}

In this chapter we make a very strong assumption:
\begin{definition}[The Realizability Assumption]
We assume that for the hypothesis class $\mathcal{H}$ the learner have chosen, there exists $h^{*} \in \mathcal{H}$ s.t. $L_{(\mathcal{D},f)}(h^{*}) = 0$, intuitively means there exists a perfect hypothesis in this hypothesis class.
\end{definition}
The realizability assumption implies that:
\begin{corollary}
For any training set $S$ sampled according to $\mathcal{D}$ and labeled by $f$, we have that $L_S(h_S) = 0$.
\end{corollary}
\begin{proof}
$L_{(\mathcal{D},f)}(h^{*}) = 0 \implies L_S(h^{*}) = 0$, which means that $\min_{h \in \mathcal{H}} L_S(h) = 0$, so $L_S(h_S) = 0$. Note that $h_S$ is not necessarily equal to $h^{*}$.
\end{proof}


\begin{observe}
    Note that when the learning algorithm have only accessed to train set $S$, any error with respect to the distribution $\mathcal{D}$ should depend on the relation between $\mathcal{D}$ and $S$. Therefore, we make the \textbf{i.i.d.} assumption:
\end{observe}
\begin{definition}[The i.i.d. assumption]
The sample in the training set are independently and identically distributed (i.i.d.) according to the distribution $\mathcal{D}$. We denote this assumption by $S \sim \mathcal{D}^m$.
\end{definition}

\begin{observe}
    Even with the i.i.d. assumption, there is still a small chance that the training set is nonrepresentative.
\end{observe}

\begin{definition}[Confidence parameter]
Denote the probability of getting a nonrepresentative sample by $\delta$, and call $(1-\delta)$ the \textbf{confidence parameter} of our prediction. 
\end{definition}

\begin{definition}[Accuracy parameter] ~

We introduce \textbf{accuracy parameter} $\epsilon$ such that we interpret
\begin{itemize}
    \item the event $L_{(\mathcal{D},f)}(h_S) > \epsilon$ as a \textbf{failure of the learner},
    \item and if $L_{(\mathcal{D},f)}(h_S) \le \epsilon$, we view the output of the algorithm as an \textbf{approximately correct predictor}.
\end{itemize}
\end{definition}

\begin{lemma}
    The upper bounding of the probability of sampling $m$ training data that leads to the failure of the learner is bounded by $|\mathcal{H}|e^{-\epsilon m}$, in another word:
    \begin{align*}
        D^{m}(\{ S|_x : L_{(\mathcal{D},f)}(h_S) > \epsilon \}) \le |\mathcal{H}|e^{-\epsilon m}
    \end{align*}
    where $S|_x = (x_1,\cdots ,x_m)$ denotes the instances of the training set.
\end{lemma}

First define:
\begin{itemize}
    \item $\mathcal{H}_B := \{ h \in \mathcal{H} : L_{(\mathcal{D},f)}(h) > \epsilon \}$ be the \textbf{set of bad hypotheses},
    \item and $M := \{ S|_x : \exists h \in \mathcal{H}_B, L_S(h) = 0\} = \bigcup_{h \in \mathcal{H}_B}\{ S|_x : L_S(h) = 0 \} $ be the \textbf{set of misleading samples}.
\end{itemize}

\begin{intuition}
    A misleading sample is the training set $S|_x$ that there exists a bad hypothesis $h$ perfectly fits $S|_x$ but does not perform well in general, that is, an easily-overfitted training set, or non-representative training set.
\end{intuition}

\begin{remark}
    Every training data instance that leads to the failure of the learner is a misleading sample:
    \begin{align*}
        \{ S|_x : L_{(\mathcal{D},f)}(h_S) > \epsilon \} \subseteq M
    \end{align*}
\end{remark}
\begin{explanation}
    For any $S|_x \in \{ S|_x : L_{(\mathcal{D},f)}(h_S) > \epsilon \}$, by the realizability assumption, we have that $L_S(h_S) = 0$. Also since $L_{(\mathcal{D},f)}(h_S) > \epsilon$, so $h_S$ is a bad hypothesis and $h_S \in \mathcal{H}_B$. We have find $h_S \in \mathcal{H}_B$ s.t. $L_S(h_S) = 0$, so $S|_x \in M$.
\end{explanation}

Now we derive:
\begin{align*}
    D^{m}(\{ S|_x : L_{(\mathcal{D},f)}(h_S) > \epsilon \}) &\le D^{m}(M) = D^{m}(\bigcup_{h \in \mathcal{H}_B}\{ S|_x : L_S(h) = 0 \})
    \\ &\le \sum\limits_{h \in \mathcal{H}_B} D^{m}(\{S|_x : L_S(h) = 0 \})
    \\ &= \sum\limits_{h \in \mathcal{H}_B} D^{m}(\{S|_x : \forall i, h(x_i) = f(x_i) \})
    \\ \text{By the i.i.d. assuptiom:}
    \\ &= \sum\limits_{h \in \mathcal{H}_B} \left[ \prod_{i = 1}^m D(\{x_i : h(x_i) = f(x_i) \}) \right]
\end{align*}
\begin{align*}
    \\ \text{Since } D(\{x_i : h(x_i) = f(x_i)\}) = 1- L_{(\mathcal{D},f)}(h):
    \\ &= \sum\limits_{h \in \mathcal{H}_B} (1- L_{(\mathcal{D},f)}(h))^{m}
    \\ \text{Since } h \in \mathcal{H}_B \text{ so } L_{(\mathcal{D},f)}> \epsilon:
    \\ &\le |\mathcal{H}_B|(1-\epsilon)^{m} \le |\mathcal{H}_B|e^{-\epsilon m} \le |\mathcal{H}|e^{-\epsilon m}
\end{align*}
This complete the proof of lemma 1.

\begin{corollary}
Let $\mathcal{H}$ be a finite hypothesis class. Let $\delta \in (0,1)$ and $\epsilon >0$, and let $m$ be an integer hat satisfies
\begin{align*}
    m \ge \frac{\log(|\mathcal{H}|/\delta)}{\epsilon}.
\end{align*}
Then for any labeling function $f$, and for any distribution $\mathcal{D}$, for which the realizability assumption holds (that is, for some $h \in \mathcal{H}$, $L_{(\mathcal{D}, f)}(h) = 0$), with probability of at least $1 - \delta$ over the choice of an i.i.d. sample $S$ of size $m$, we have that for every ERM hypothesis, $h_S$, it holds that
\begin{align*}
    L_{(\mathcal{D}, f)}(h_S) \le \epsilon.
\end{align*}
\end{corollary}
\begin{proof}
    By the lemma above,
    \begin{align*}
        D^{m}(\{ S|_x : L_{(\mathcal{D},f)}(h_S) > \epsilon \}) &\le |\mathcal{H}|e^{-\epsilon m}
        \\ &\le |\mathcal{H}|e^{-\epsilon \frac{\log(|\mathcal{H}|/\delta)}{\epsilon}} = \delta,
    \end{align*}
    which completes the proof.
\end{proof}

\begin{intuition}
    This tells us that for a sufficiently large training set, we are $(1-\delta)$ confident that the probability for our $\text{ERM}_\mathcal{H}$ hypothesis making errors on a randomly chosen data is no larger than $\epsilon$.
\end{intuition}

\subsection{Exercise} \label{sec:}
\Problem Let $\mathcal{H}$ be a class of binary classifier over a domain $\mathcal{X}$. Let $\mathcal{D}$ be an unknown distribution over $\mathcal{X}$, and let $f$ be the target hypothesis in $\mathcal{H}$. Fix some $h \in \mathcal{H}$. Show that the expected value of $L_S(h)$ over the choice of $S|_x$ equals $L_{(\mathcal{D},f)}(h)$, namely,
\begin{align*}
    \underset{S|_x \sim \mathcal{D}^{m}}{\mathbb{E}}[L_S(h)] = L_{(\mathcal{D},f)}(h).
\end{align*}

\TheSolution \begin{align*}
    \underset{S|_x \sim \mathcal{D}^{m}}{\mathbb{E}}[L_S(h)] &= \sum\limits_{i = 1}^{m}\frac{i}{m} {m \choose i} \mathcal{D}(\{ x : h(x) = f(x) \})^{m-i} \mathcal{D}(\{ x : h(x) \neq f(x) \})^{i} 
    \\ &= \sum\limits_{i = 1}^{m} \frac{(m-1)!}{(i-1)!(m-i)!} (1-L_{(\mathcal{D},f)}(h))^{m-i} L_{(\mathcal{D},f)}(h)^{i}
    \\ &= \sum\limits_{k = 0}^{n} \frac{n!}{k!(n-k)!} (1-L_{(\mathcal{D},f)}(h))^{n-k} L_{(\mathcal{D},f)}(h)^{k + 1} = L_{(\mathcal{D},f)}(h)
\end{align*}

\newpage
\section{PAC learning model} \label{sec:}
\begin{definition}[PAC Learnability]
    A hypothesis class $\mathcal{H}$ is \textbf{Probably Approximately Correct (PAC) Learnable} if there exist a function $m_\mathcal{H}: (0,1)^{2} \to \mathbb{N}$ and a learning algorithm such that:
    \begin{itemize}
        \item For every $\epsilon, \delta \in (0,1)$,
        \item for every distribution $\mathcal{D}$ over $\mathcal{X}$,
        \item and for every labeling function $f:\mathcal{X} \to \{ 0,1 \}$,
    \end{itemize}
    and if the realizable assumption holds with respect to $\mathcal{H}$, $\mathcal{D}$, $f$,
    \\ then when running the learning algorithm on $m \ge m_\mathcal{H}(\epsilon,\delta)$ i.i.d. examples generated by $\mathcal{D}$ and labeled by $f$, the algorithm returns a hypothesis $h$ s.t. with probability of at least $1-\delta$ (over the choice of the examples), $L_{(\mathcal{D},f)}(h) \le \epsilon$, and such hypothesis $h$ is a \textbf{probably approximately correct solution}.
\end{definition}

\begin{definition}[Sample Complexity]
\textbf{Sample complexity} $m_\mathcal{H}: (0,1)^{2} \to \mathbb{N}$ is the function of $\epsilon$ and $\delta$ that returns the minimal size of training set needed to guarantee a probably approximately correct solution.
\end{definition}

\begin{remark}
$m_\mathcal{H}$ also depend on $\mathcal{H}$, for example when the hypothesis class is finite, $m_\mathcal{H}$ is proportional to $\log |\mathcal{H}|$.
\end{remark}

\begin{corollary}
Every finite hypothesis class is PAC learnable with sample complexity
\begin{align*}
    m_{\mathcal{H}}(\epsilon, \delta) \le \left\lceil \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} \right\rceil 
\end{align*}
\end{corollary}

\begin{intuition}
    Since we know that when the sample size is larger that $\left\lceil \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} \right\rceil $, the resulting hypothesis is a probably approximately correct solution, so the minimal sample size needed is no larger than this value.
\end{intuition}

\subsection{Agnostic PAC learning} \label{sec:}
